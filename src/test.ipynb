{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Sau hai thế kỷ, chính sách đóng cửa đất nước dưới thời Mạc phủ Tokugawa đã đi đến kết thúc khi Nhật Bản bị Hoa Kỳ ép mở cửa giao thương vào năm 1854. Những năm tiếp theo cuộc Minh Trị duy tân năm 1868 và sự sụp đổ của chế độ mạc phủ, Nhật Bản đã tự chuyển đổi từ một xã hội khá lạc hậu và phong kiến sang một quốc gia công nghiệp hiện đại. Nhật đã cử các phái đoàn và sinh viên đi khắp thế giới để học và tiếp thu khoa học và nghệ thuật phương Tây, điều này đã được thực hiện nhằm giúp Nhật Bản tránh khỏi rơi vào ách thống trị của nước ngoài và cũng giúp cho Nhật có thể cạnh tranh ngang ngửa với các cường quốc phương Tây.\","
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INIT VNCORENLP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VnCoreNLP model folder D:\\Study\\DATN\\model\\NKD-GNN-test\\VnCoreNLP already exists! Please load VnCoreNLP from this folder!\n"
     ]
    }
   ],
   "source": [
    "import py_vncorenlp\n",
    "from word_lists import PEOPLE, PLACE, ORGANIZATION, number_map\n",
    "# Automatically download VnCoreNLP components from the original repository\n",
    "py_vncorenlp.download_model(save_dir=r'D:\\Study\\DATN\\model\\NKD-GNN-test\\VnCoreNLP')\n",
    "\n",
    "# Load VnCoreNLP from the local working folder that contains both `VnCoreNLP-1.2.jar` and `models`\n",
    "model_vncore = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\", \"pos\", \"ner\"], save_dir=r'D:\\Study\\DATN\\model\\NKD-GNN-test\\VnCoreNLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_template_caption(text, model_vncore):\n",
    "    PERSON_PLACEHOLDER = \"<PERSON>\"\n",
    "    PLACE_PLACEHOLDER = \"<PLACE>\"\n",
    "    ORGANIZATION_PLACEHOLDER = \"<ORGANIZATION>\"\n",
    "    # Split into sentences\n",
    "    sentences = text.split(\".\")\n",
    "\n",
    "    # Annotate the chunk with VnCoreNLP\n",
    "    caption = []\n",
    "    for splited_sentence in sentences:\n",
    "        annotations = model_vncore.annotate_text(splited_sentence)\n",
    "\n",
    "        # Initialize a list to store the processed tokens\n",
    "        result = []\n",
    "\n",
    "        # Variables to handle multi-token entities\n",
    "        current_entity = None\n",
    "        current_placeholder = None\n",
    "\n",
    "        # Loop over each sentence in the annotation result (annotations is a dictionary with indices as keys)\n",
    "        for sentence_key in annotations:\n",
    "            sentence = annotations[sentence_key]\n",
    "            #To reduce from 2-3 posTag \"N\" sit next together only return 1 \"N\" tag\n",
    "            flag = False\n",
    "            # Process each token in the sentence\n",
    "            prev_word = \"\"\n",
    "            prev_pos_tag = \"\"\n",
    "            for token in sentence:\n",
    "                word = token['wordForm']\n",
    "                pos_tag = token['posTag']\n",
    "                if pos_tag == 'N' and flag == False:\n",
    "                    if word.lower() in PEOPLE:\n",
    "                        if prev_pos_tag == 'M':\n",
    "                            try:\n",
    "                                num_person = number_map[prev_word.lower()]\n",
    "                            except:\n",
    "                                num_person = 1\n",
    "                            result.pop()\n",
    "                            for i in range(num_person - 1):\n",
    "                                result.append(PERSON_PLACEHOLDER + \",\")\n",
    "                        result.append(PERSON_PLACEHOLDER)\n",
    "                    elif word.lower() in PLACE:\n",
    "                        result.append(PLACE_PLACEHOLDER)\n",
    "                    elif word.lower() in ORGANIZATION:\n",
    "                        result.append(ORGANIZATION_PLACEHOLDER)\n",
    "                    else:\n",
    "                        result.append(word)\n",
    "                    flag = True\n",
    "                elif pos_tag == 'N' and flag == True:\n",
    "                    continue\n",
    "                elif pos_tag !='Nc' and pos_tag != 'P':\n",
    "                    result.append(word)\n",
    "                    flag = False\n",
    "                    prev_word = word\n",
    "                    prev_pos_tag = pos_tag\n",
    "\n",
    "        caption.append(' '.join(result))\n",
    "    # Join the processed tokens back into a sentence\n",
    "    return '. '.join(caption).strip(\" .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Sau hai thế kỷ, chính sách đóng cửa đất nước dưới thời Mạc phủ Tokugawa đã đi đến kết thúc khi Nhật Bản bị Hoa Kỳ ép mở cửa giao thương vào năm 1854. Những năm tiếp theo cuộc Minh Trị duy tân năm 1868 và sự sụp đổ của chế độ mạc phủ, Nhật Bản đã tự chuyển đổi từ một xã hội khá lạc hậu và phong kiến sang một quốc gia công nghiệp hiện đại. Nhật đã cử các phái đoàn và sinh viên đi khắp thế giới để học và tiếp thu khoa học và nghệ thuật phương Tây, điều này đã được thực hiện nhằm giúp Nhật Bản tránh khỏi rơi vào ách thống trị của nước ngoài và cũng giúp cho Nhật có thể cạnh tranh ngang ngửa với các cường quốc phương Tây.\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"Nguyễn Sinh Phúc và Nguyễn Phúc Trọng đang bắt tay nhau tại Tòa nhà Quốc hội. Hai người đàn ông đang đánh nhau ở quảng trường\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nguyễn Sinh Phúc và Nguyễn Phúc Trọng đang bắt tay nhau tại Tòa nhà Quốc hội. Hai người đàn ông đang đánh nhau ở quảng trường\n",
      "Nguyễn_Sinh_Phúc và Nguyễn_Phúc_Trọng đang bắt_tay nhau tại <PLACE>. <PERSON>, <PERSON> đang đánh nhau ở <ORGANIZATION>\n"
     ]
    }
   ],
   "source": [
    "print(generate_template_caption(text = test_str, model_vncore=model_vncore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAW CAPTION GENERATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/f5/57/f2e6568dbf464a4b270954e5fa3dee4a4054d163a41c0e7bf0a34eb40f0f/gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in d:\\study\\datn\\model\\nkd-gnn-env\\lib\\site-packages (from gensim) (1.26.3)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Obtaining dependency information for scipy<1.14.0,>=1.7.0 from https://files.pythonhosted.org/packages/4a/48/4513a1a5623a23e95f94abd675ed91cfb19989c58e9f6f7d03990f6caf3d/scipy-1.13.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached scipy-1.13.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Obtaining dependency information for smart-open>=1.8.1 from https://files.pythonhosted.org/packages/06/bc/706838af28a542458bffe74a5d0772ca7f207b5495cd9fccfce61ef71f2a/smart_open-7.0.5-py3-none-any.whl.metadata\n",
      "  Using cached smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Obtaining dependency information for wrapt from https://files.pythonhosted.org/packages/63/bb/c293a67fb765a2ada48f48cd0f2bb957da8161439da4c03ea123b9894c02/wrapt-1.17.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading wrapt-1.17.0-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Using cached gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "Using cached scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "Using cached smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.17.0-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Installing collected packages: wrapt, scipy, smart-open, gensim\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed gensim-4.3.3 scipy-1.13.1 smart-open-7.0.5 wrapt-1.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gensim\n",
    "\n",
    "def load_pretrained_embeddings(vocab, embedding_dim, embedding_file):\n",
    "    \"\"\"\n",
    "    Load pretrained embeddings and create a weights matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - vocab: Dictionary mapping words to indices in the vocabulary\n",
    "    - embedding_dim: Dimension of the embeddings\n",
    "    - embedding_file: Path to the pretrained embedding file (e.g., .vec or .txt)\n",
    "\n",
    "    Returns:\n",
    "    - weights_matrix: A matrix of shape (vocab_size, embedding_dim)\n",
    "    \"\"\"\n",
    "    # Load pretrained embeddings using gensim\n",
    "    embeddings = gensim.models.KeyedVectors.load_word2vec_format(embedding_file, binary=False)\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    weights_matrix = torch.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    # Fill weights matrix with pretrained embeddings\n",
    "    for word, idx in vocab.items():\n",
    "        if word in embeddings:\n",
    "            weights_matrix[idx] = torch.tensor(embeddings[word])\n",
    "        else:\n",
    "            # Randomly initialize missing words\n",
    "            weights_matrix[idx] = torch.randn(embedding_dim)\n",
    "\n",
    "    return weights_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from preprocess_image import resize_image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################## TEMPLATE CAPTION GENERATOR ###############################\n",
    "\n",
    "# CNN Encoder with Pre-trained\n",
    "class VGG19Encoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(VGG19Encoder, self).__init__()\n",
    "        # Load pre-trained\n",
    "        vgg = models.vgg19(pretrained=True)\n",
    "        # Remove the last fully connected layer (fc) of \n",
    "        self.vgg = nn.Sequential(*list(vgg.children())[:-1])\n",
    "        # Linear layer to map ResNet output to embedding size\n",
    "        self.fc = nn.Linear(vgg.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Extract features using \n",
    "        with torch.no_grad():  # Disable gradient computation for ResNet\n",
    "            features = self.vgg(images)\n",
    "        # Flatten the features and pass through a linear layer\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.fc(features)\n",
    "        features = self.relu(features)\n",
    "        features = self.dropout(features)\n",
    "        return features\n",
    "    \n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, pretrained_embeddings=None):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # Load pretrained embeddings if provided\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            self.embedding.weight.requires_grad = False  # Freeze embeddings if desired\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embedding(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.fc(hiddens)\n",
    "        return outputs\n",
    "    \n",
    "# Full model combining CNN encoder and LSTM decoder\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, embed_size=256, hidden_size=512, vocab_size=10000, num_layers=1, embedding = None):\n",
    "        super().__init__()\n",
    "        self.encoder = VGG19Encoder(embed_size)\n",
    "        self.decoder = LSTMDecoder(embed_size, hidden_size, vocab_size, num_layers,embedding)\n",
    "\n",
    "    def forward(self, images, embedding):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, embedding)\n",
    "        return outputs\n",
    "\n",
    "    def generate_caption(self, image, max_length=20, vocab=None):\n",
    "        # Preprocess the image\n",
    "        image = resize_image(image)\n",
    "        features = self.encoder(image)\n",
    "\n",
    "        # Start decoding\n",
    "        captions = [vocab['<START>']]  # Begin with the start token\n",
    "        for _ in range(max_length):\n",
    "            caption_tensor = torch.tensor(captions).unsqueeze(0).to(features.device)\n",
    "            outputs = self.decoder(features, caption_tensor)\n",
    "            next_word_id = outputs.argmax(2)[:, -1].item()  # Get the predicted next word\n",
    "            captions.append(next_word_id)\n",
    "            if vocab and next_word_id == vocab['<END>']:\n",
    "                break\n",
    "        return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example vocabulary\n",
    "vocab = {'<PAD>': 0, '<START>': 1, '<END>': 2, 'tôi': 3, 'đi': 4, 'học': 5}  # Example vocab\n",
    "embedding_dim = 300\n",
    "embedding_file = \"cc.vi.300.vec\"  # Path to FastText Vietnamese embeddings\n",
    "\n",
    "pretrained_weights = load_pretrained_embeddings(vocab, embedding_dim, embedding_file)\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = ImageCaptioningModel(embed_size=256, hidden_size=512, vocab_size=len(vocab))\n",
    "\n",
    "# Load an image and generate a caption\n",
    "image_path = \"path/to/image.jpg\"\n",
    "caption_ids = model.generate_caption(image_path, vocab=vocab)\n",
    "\n",
    "# Convert caption IDs to words\n",
    "reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "caption = ' '.join(reverse_vocab[id] for id in caption_ids if id in reverse_vocab)\n",
    "print(\"Generated Caption:\", caption)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NKD-GNN-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
